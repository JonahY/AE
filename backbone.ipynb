{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-03T01:27:14.215847Z",
     "start_time": "2020-11-03T01:27:14.199829Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import time\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "\n",
    "%matplotlib qt5\n",
    "plt.rcParams['xtick.direction'] = 'in'\n",
    "plt.rcParams['ytick.direction'] = 'in'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-03T02:00:29.845402Z",
     "start_time": "2020-11-03T02:00:29.831401Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def plot(energy, inter, mid, edgecolor, idx = 0):\n",
    "    x = np.array([])\n",
    "    # 初始化横坐标\n",
    "    for i in inter:\n",
    "        if i != 0:\n",
    "            x = np.append(x, np.linspace(i, i * 10, 8, endpoint=False))\n",
    "        else:\n",
    "            x = np.append(x, np.linspace(i, 1, 8, endpoint=False))\n",
    "    \n",
    "    # 初始化纵坐标\n",
    "    y = np.zeros(x.shape[0])\n",
    "    for i in energy:\n",
    "        while True:\n",
    "            if x[idx] <= i < x[idx + 1]:\n",
    "                y[idx] += 1\n",
    "                break\n",
    "            idx += 1\n",
    "    \n",
    "    # 对横坐标作进一步筛选，计算概率分布值\n",
    "    x, y = x[y != 0], y[y != 0]\n",
    "    xx = np.zeros(x.shape[0])\n",
    "    yy = y / sum(y)\n",
    "    \n",
    "    # 取区间终点作为该段的横坐标\n",
    "    for idx in range(len(x) - 1):\n",
    "        xx[idx] = (x[idx] + x[idx + 1]) / 2\n",
    "    xx[-1] = x[-1]\n",
    "    \n",
    "    # 计算分段区间长度，从而求得概率密度值\n",
    "    interval = []\n",
    "    for i, j in enumerate(mid):\n",
    "        try:\n",
    "            num = len(np.intersect1d(np.where(inter[i] <= xx)[0], np.where(xx < inter[i + 1])[0]))\n",
    "            interval.extend([j] * num)\n",
    "        except IndexError:\n",
    "            num = len(np.where(inter[i] <= xx)[0])\n",
    "            interval.extend([j] * num)\n",
    "    yy = yy / np.array(interval)\n",
    "    \n",
    "    # 取对数变换为线性关系\n",
    "    xx = np.log10(xx)\n",
    "    yy = np.log10(yy)\n",
    "    plt.scatter(xx, yy, edgecolors=edgecolor)\n",
    "    return xx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-03T02:00:34.579666Z",
     "start_time": "2020-11-03T02:00:34.215663Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    features_path = r'C:\\Users\\Yuan\\Desktop\\pri_database.txt'\n",
    "    label_path = r'C:\\Users\\Yuan\\Desktop\\label.txt'\n",
    "\n",
    "    # Amp,RiseT,Dur,Eny,RMS,Counts\n",
    "    with open(features_path, 'r') as f:\n",
    "        feature = np.array([i.split(',')[6:-4] for i in f.readlines()[1:]])\n",
    "    feature = feature.astype(np.float32)\n",
    "\n",
    "    with open(label_path, 'r') as f:\n",
    "        label = np.array([i.strip() for i in f.readlines()[1:]])\n",
    "    label = label.astype(np.float32).reshape(-1, 1)\n",
    "    label[np.where(label == 2)] = 0\n",
    "    ext = np.zeros([feature.shape[0], 1])\n",
    "    ext[np.where(label == 0)[0].tolist()] = 1\n",
    "    label = np.concatenate((label, ext), axis=1)\n",
    "\n",
    "    feature_idx = [0, 2, 3]\n",
    "    interz = [[pow(10, i) for i in range(1, 5)],\n",
    "              [pow(10, i) for i in range(4)],\n",
    "              [0] + [pow(10, i) for i in range(6)]]\n",
    "    midz = [[0.125 * pow(10, i) for i in range(2, 6)],\n",
    "            [0.125 * pow(10, i) for i in range(1, 5)],\n",
    "            [0.125 * pow(10, i) for i in range(7)]]\n",
    "    xlabelz = ['Amplitude(μV)', 'Duration(μs)', 'Energy(aJ)']\n",
    "    ylabelz = ['PDF(A)', 'PDF(D)', 'PDF(E)']\n",
    "    cls_1 = label[:, 0] == 1\n",
    "    cls_2 = label[:, 1] == 1\n",
    "\n",
    "    fig = plt.figure()\n",
    "    for i, [idx, inter, mid, xlabel, ylabel\n",
    "            ] in enumerate(zip(feature_idx, interz, midz, xlabelz, ylabelz)):\n",
    "        ax = fig.add_subplot(321 + i)\n",
    "        tmp = feature[:, idx]\n",
    "        tmp_1, tmp_2 = sorted(tmp[cls_1]), sorted(tmp[cls_2])\n",
    "        #     xx = plot(sorted(energy), inter, mid, 'blue')\n",
    "        xx = plot(tmp_1, inter, mid, 'purple')\n",
    "        xx = plot(tmp_2, inter, mid, 'g')\n",
    "        ax.set_xlabel(xlabel)\n",
    "        ax.set_ylabel(ylabel)\n",
    "\n",
    "    cor_idx = [[0, 3], [2, 0], [2, 3]]\n",
    "    for idx, [i, j] in enumerate(cor_idx):\n",
    "        ax = fig.add_subplot(321 + idx + 3)\n",
    "        cor_x = np.log10(feature[:, i])\n",
    "        cor_y = np.log10(feature[:, j])\n",
    "        cor_x1, cor_x2 = sorted(cor_x[cls_1]), sorted(cor_x[cls_2])\n",
    "        cor_y1, cor_y2 = sorted(cor_y[cls_1]), sorted(cor_y[cls_2])\n",
    "        ax.scatter(cor_x1, cor_y1, edgecolors='purple')\n",
    "        ax.scatter(cor_x2, cor_y2, edgecolors='g')\n",
    "        ax.set_xlabel(xlabelz[max(0, i - 1)])\n",
    "        ax.set_ylabel(xlabelz[max(0, j - 1)])\n",
    "\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-02T09:29:26.691454Z",
     "start_time": "2020-11-02T09:29:26.671424Z"
    }
   },
   "outputs": [],
   "source": [
    "# xx[-1]\n",
    "# 9, 5, 4, 2, 1, 0.8, 0.75, 0.5, 0.5\n",
    "base = np.array([9, 14, 18, 20, 21, 21.8, 22.55, 23.05, 23.55])\n",
    "tick_1 = base + 0\n",
    "tick_2 = base + tick_1[-1]\n",
    "tick_3 = base + tick_2[-1]\n",
    "tick_4 = base + tick_3[-1]\n",
    "tick_5 = base + tick_4[-1]\n",
    "tick_6 = base + tick_5[-1]\n",
    "x_tick = np.concatenate((tick_1, tick_2, tick_3, tick_4, tick_5, tick_6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-30T00:42:58.921178Z",
     "start_time": "2020-10-30T00:42:58.866841Z"
    }
   },
   "outputs": [],
   "source": [
    "pca = PCA(n_components=5)\n",
    "newData = pca.fit_transform(feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-30T01:00:11.470400Z",
     "start_time": "2020-10-30T01:00:11.462370Z"
    }
   },
   "outputs": [],
   "source": [
    "pca.components_.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import time\n",
    "import argparse\n",
    "import torch\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-30T02:58:54.931743Z",
     "start_time": "2020-10-30T02:58:54.906715Z"
    }
   },
   "outputs": [],
   "source": [
    "def metric(logit, truth, threshold=0.5):\n",
    "    batch_size, num_class = logit.shape\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logit = logit.view(batch_size, num_class, -1)\n",
    "        truth = truth.view(batch_size, num_class, -1)\n",
    "\n",
    "        probability = torch.sigmoid(logit)\n",
    "        p = (probability > threshold).float()\n",
    "        t = (truth > 0.5).float()\n",
    "\n",
    "        tp = ((p + t) == 2).float()  # True positives\n",
    "        tn = ((p + t) == 0).float()  # True negatives\n",
    "        # 各个类别预测正确的正样本、负样本数目\n",
    "        tp = tp.sum(dim=0)\n",
    "        tn = tn.sum(dim=0)\n",
    "        num_pos = t.sum(dim=0)\n",
    "        num_neg = batch_size - num_pos\n",
    "        # 预测正确的正样本和负样本的数目\n",
    "        tp = tp.data.cpu().numpy()\n",
    "        tn = tn.data.cpu().numpy()\n",
    "        # 正样本、负样本的数目\n",
    "        num_pos = num_pos.data.cpu().numpy()\n",
    "        num_neg = num_neg.data.cpu().numpy()\n",
    "\n",
    "        # tp = np.nan_to_num(tp / (num_pos + 1e-12), 0)\n",
    "        # tn = np.nan_to_num(tn / (num_neg + 1e-12), 0)\n",
    "\n",
    "        # tp = list(tp)\n",
    "        # num_pos = list(num_pos)\n",
    "\n",
    "    return tn, tp, num_neg, num_pos\n",
    "\n",
    "\n",
    "class Meter:\n",
    "    '''A meter to keep track of iou and dice scores throughout an epoch'''\n",
    "    def __init__(self):\n",
    "        self.base_threshold = 0.5\n",
    "        self.true_negative = []\n",
    "        self.true_poisitive = []\n",
    "        self.number_negative = []\n",
    "        self.number_positive = []\n",
    "\n",
    "    def update(self, targets, outputs):\n",
    "        tn, tp, num_neg, num_pos = metric(outputs, targets, self.base_threshold)\n",
    "        self.true_negative.append(tn)\n",
    "        self.true_poisitive.append(tp)\n",
    "        self.number_negative.append(num_neg)\n",
    "        self.number_positive.append(num_pos)\n",
    "\n",
    "    def get_metrics(self):\n",
    "        # 各类预测正确的样本数目，样本总数目\n",
    "        class_tn = np.sum(np.array(self.true_negative), axis=0)\n",
    "        class_tp = np.sum(np.array(self.true_poisitive), axis=0)\n",
    "        class_num_neg = np.sum(np.array(self.number_negative), axis=0)\n",
    "        class_num_pos = np.sum(np.array(self.number_positive), axis=0)\n",
    "        # 预测正确的样本的总数目，样本总数目\n",
    "        tn = np.sum(self.true_negative)\n",
    "        tp = np.sum(self.true_poisitive)\n",
    "        num_neg = np.sum(self.number_negative)\n",
    "        num_pos = np.sum(self.number_positive)\n",
    "        # 各类的正负样本的准确率和总的准确率\n",
    "        class_neg_accuracy = class_tn / class_num_neg\n",
    "        class_pos_accuracy = class_tp / class_num_pos\n",
    "        class_accuracy = (class_tn + class_tp) / (class_num_neg + class_num_pos)\n",
    "        # 正负样本各自的准确率和总的准确率\n",
    "        neg_accuracy = tn / (num_neg + 1e-12)\n",
    "        pos_accuracy = tp / (num_pos + 1e-12)\n",
    "        accuracy = (tn + tp) / (num_neg + num_pos)\n",
    "\n",
    "        return class_neg_accuracy, class_pos_accuracy, class_accuracy, neg_accuracy, pos_accuracy, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-30T02:31:26.350549Z",
     "start_time": "2020-10-30T02:31:26.331555Z"
    }
   },
   "outputs": [],
   "source": [
    "class Classify_model(torch.nn.Module):\n",
    "    def __init__(self, layer, training=True):\n",
    "        super(Fit_model,self).__init__()\n",
    "        self.linear1 = torch.nn.Linear(layer[0],layer[1])\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.linear2 = torch.nn.Linear(layer[1],layer[2])\n",
    "        self.linear3 = torch.nn.Linear(layer[2],1)\n",
    "        self.criterion = torch.nn.MSELoss()\n",
    "        self.sigmoid = torch.nn.Sigmoid() \n",
    "        self.opt = torch.optim.SGD(self.parameters(),lr=0.0001)\n",
    "        self.training = training\n",
    "    def forward(self, input):\n",
    "        y = self.linear1(input)\n",
    "        y = self.relu(y)\n",
    "        y = F.dropout(y, 0.5, training=self.training)\n",
    "        y = self.linear2(y)\n",
    "        y = self.sigmoid(y)\n",
    "        y = self.linear3(y)\n",
    "        y = self.sigmoid(y)\n",
    "        return y "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-30T01:54:17.434672Z",
     "start_time": "2020-10-30T01:54:17.424642Z"
    }
   },
   "outputs": [],
   "source": [
    "class SteelClassDataset(Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        super(SteelClassDataset, self).__init__()\n",
    "        self.feature = dataset[0]\n",
    "        self.label = dataset[1]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.feature[idx]\n",
    "        y = self.label[idx]\n",
    "        return x, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-30T02:46:37.616990Z",
     "start_time": "2020-10-30T02:46:37.593021Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def classify_provider(features_path, label_path, n_splits, batch_size,num_workers):\n",
    "    # Time,Thr,Amp,RiseT,Dur,Eny,RMS,Counts\n",
    "    with open(features_path, 'r') as f:\n",
    "        feature = np.array([i.split(',')[6:-4] for i in f.readlines()[1:]])\n",
    "    # feature = np.delete(feature, [1, 2], 1).astype(np.float32)\n",
    "    feature = torch.from_numpy(feature.astype(np.float32))\n",
    "\n",
    "    with open(label_path, 'r') as f:\n",
    "        label = np.array([i.strip() for i in f.readlines()[1:]])\n",
    "    label = label.astype(np.float32)\n",
    "    label[np.where(label == 2)] = 0\n",
    "    label = torch.unsqueeze(torch.from_numpy(label), dim=1)\n",
    "    \n",
    "    train_dfs = list()\n",
    "    val_dfs = list()\n",
    "    if n_splits != 1:\n",
    "        skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=69)\n",
    "        for train_df_index, val_df_index in skf.split(feature, label):\n",
    "            train_dfs.append([feature[train_df_index], label[train_df_index]])\n",
    "            val_dfs.append([feature[val_df_index], label[val_df_index]])\n",
    "    else:\n",
    "        df_temp = train_test_split(feature,label, test_size=0.2, stratify=label, random_state=69)\n",
    "        train_dfs.append([df_temp[0], df_temp[2]])\n",
    "        val_dfs.append([df_temp[1], df_temp[3]])\n",
    "\n",
    "    dataloaders = list()\n",
    "    for df_index, (train_df, val_df) in enumerate(zip(train_dfs, val_dfs)):\n",
    "        train_dataset = SteelClassDataset(train_df)\n",
    "        val_dataset = SteelClassDataset(val_df)\n",
    "        train_dataloader = DataLoader(train_dataset,\n",
    "                                      batch_size=batch_size,\n",
    "                                      num_workers=num_workers,\n",
    "                                      pin_memory=True,\n",
    "                                      shuffle=True)\n",
    "        val_dataloader = DataLoader(val_dataset,\n",
    "                                    batch_size=batch_size,\n",
    "                                    num_workers=num_workers,\n",
    "                                    pin_memory=True,\n",
    "                                    shuffle=False)\n",
    "        dataloaders.append([train_dataloader, val_dataloader])\n",
    "    return dataloaders\n",
    "\n",
    "\n",
    "class Solver():\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.device = torch.device(\n",
    "            'cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    def forward(self, images):\n",
    "        images = images.to(self.device)\n",
    "        outputs = self.model(images)\n",
    "        return outputs\n",
    "\n",
    "    def cal_loss(self, targets, predicts, criterion):\n",
    "        targets = targets.to(self.device)\n",
    "        return criterion(predicts, targets)\n",
    "\n",
    "    def backword(self, optimizer, loss):\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    def save_checkpoint(self, save_path, state, is_best):\n",
    "        torch.save(state, save_path)\n",
    "        if is_best:\n",
    "            print('Saving Best Model.')\n",
    "            save_best_path = save_path.replace('.pth', '_best.pth')\n",
    "            shutil.copyfile(save_path, save_best_path)\n",
    "\n",
    "    def load_checkpoint(self, load_path):\n",
    "        if os.path.isfile(load_path):\n",
    "            checkpoint = torch.load(load_path, map_location='cpu')\n",
    "            # self.model.module.load_state_dict(checkpoint['state_dict'])\n",
    "            print('Successfully Loaded from %s' % (load_path))\n",
    "            return self.model\n",
    "        else:\n",
    "            raise FileNotFoundError(\n",
    "                \"Can not find weight file in {}\".format(load_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-30T02:47:08.770237Z",
     "start_time": "2020-10-30T02:47:08.752270Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class TrainVal():\n",
    "    def __init__(self, config, fold):\n",
    "        self.model = Classify_model(config.layer, training=True)\n",
    "        if torch.cuda.is_available():\n",
    "            self.model = torch.nn.DataParallel(self.model)\n",
    "            self.model = self.model.cuda()\n",
    "        \n",
    "        self.lr = config.lr\n",
    "        self.weight_decay = config.weight_decay\n",
    "        self.epoch = config.epoch\n",
    "        self.fold = fold\n",
    "\n",
    "        self.solver = Solver(self.model)\n",
    "        self.criterion = torch.nn.MSELoss()\n",
    "        \n",
    "        self.model_path = config.save_path\n",
    "        if not os.path.exists(self.model_path):\n",
    "            os.makedirs(self.model_path)\n",
    "            \n",
    "    def train(self, train_loader, valid_loader):\n",
    "        optimizer = optim.Adam(self.model.module.parameters(), self.lr, weight_decay=self.weight_decay)\n",
    "        lr_scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, self.epoch+10)\n",
    "        global_step = 0\n",
    "\n",
    "        for epoch in range(self.epoch):\n",
    "            epoch += 1\n",
    "            epoch_loss = 0\n",
    "            self.model.train(True)\n",
    "\n",
    "            tbar = tqdm.tqdm(train_loader)\n",
    "            for i, (x, labels) in enumerate(tbar):\n",
    "                labels_predict = self.solver.forward(x)\n",
    "                loss = self.solver.cal_loss(labels, labels_predict, self.criterion)\n",
    "                epoch_loss += loss.item()\n",
    "                self.solver.backword(optimizer, loss)\n",
    "                \n",
    "                params_groups_lr = str()\n",
    "                for group_ind, param_group in enumerate(optimizer.param_groups):\n",
    "                    params_groups_lr = params_groups_lr + 'params_group_%d' % (group_ind) + ': %.12f, ' % (param_group['lr'])\n",
    "                descript = \"Fold: %d, Train Loss: %.7f, lr: %s\" % (self.fold, loss.item(), params_groups_lr)\n",
    "                tbar.set_description(desc=descript)\n",
    "            \n",
    "            lr_scheduler.step()\n",
    "            global_step += len(train_loader)\n",
    "\n",
    "            print('Finish Epoch [%d/%d], Average Loss: %.7f' % (epoch, self.epoch, epoch_loss/len(tbar)))\n",
    "            \n",
    "            class_neg_accuracy, class_pos_accuracy, class_accuracy, neg_accuracy, pos_accuracy, accuracy, loss_valid = \\\n",
    "                self.validation(valid_loader)\n",
    "\n",
    "            if accuracy > self.max_accuracy_valid: \n",
    "                is_best = True\n",
    "                self.max_accuracy_valid = accuracy\n",
    "            else:\n",
    "                is_best = False\n",
    "            \n",
    "            state = {\n",
    "                'epoch': epoch,\n",
    "                'state_dict': self.model.module.state_dict(),\n",
    "                'max_accuracy_valid': self.max_accuracy_valid,\n",
    "            }\n",
    "            \n",
    "            self.solver.save_checkpoint(os.path.join(self.model_path, '%s_classify_fold%d.pth' % (self.model_name, self.fold)), state, is_best)\n",
    "            self.writer.add_scalar('valid_loss', loss_valid, epoch)\n",
    "            self.writer.add_scalar('valid_accuracy', accuracy, epoch)\n",
    "            self.writer.add_scalar('valid_class_0_accuracy', class_accuracy[0], epoch)\n",
    "            self.writer.add_scalar('valid_class_1_accuracy', class_accuracy[1], epoch)\n",
    "            \n",
    "    def validation(self, valid_loader):\n",
    "        self.model.eval()\n",
    "        meter = Meter()\n",
    "        tbar = tqdm.tqdm(valid_loader)\n",
    "        loss_sum = 0\n",
    "        \n",
    "        with torch.no_grad(): \n",
    "            for i, (x, labels) in enumerate(tbar):\n",
    "                labels_predict = self.solver.forward(x)\n",
    "                loss = self.solver.cal_loss(labels, labels_predict, self.criterion)\n",
    "                loss_sum += loss.item()\n",
    "\n",
    "                meter.update(labels, labels_predict.cpu())\n",
    "\n",
    "                descript = \"Val Loss: {:.7f}\".format(loss.item())\n",
    "                tbar.set_description(desc=descript)\n",
    "        loss_mean = loss_sum / len(tbar)\n",
    "        \n",
    "        class_neg_accuracy, class_pos_accuracy, class_accuracy, neg_accuracy, pos_accuracy, accuracy = meter.get_metrics()\n",
    "        print(\"Class_0_accuracy: %0.4f | Class_1_accuracy: %0.4f | Negative accuracy: %0.4f | positive accuracy: %0.4f | accuracy: %0.4f\" %\n",
    "              (class_accuracy[0], class_accuracy[1], neg_accuracy, pos_accuracy, accuracy))\n",
    "        return class_neg_accuracy, class_pos_accuracy, class_accuracy, neg_accuracy, pos_accuracy, accuracy, loss_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-30T02:59:36.525403Z",
     "start_time": "2020-10-30T02:59:36.496375Z"
    }
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--features_path', type=str, default=r'C:\\Users\\Yuan\\Desktop\\pri_database.txt')\n",
    "    parser.add_argument('--label_path', type=str, default=r'C:\\Users\\Yuan\\Desktop\\label.txt')\n",
    "    parser.add_argument('--save_path', type=str, default='./checkpoints')\n",
    "    parser.add_argument('--class_num', type=int, default=4)\n",
    "    parser.add_argument('--num_workers', type=int, default=8)\n",
    "    parser.add_argument('--lr', type=float, default=5e-5, help='init lr')\n",
    "    parser.add_argument('--weight_decay', type=float, default=0, help='weight_decay in optimizer')\n",
    "    parser.add_argument('--n_splits', type=int, default=5, help='n_splits_fold')\n",
    "    parser.add_argument('--batch_size', type=int, default=24, help='batch size')\n",
    "    parser.add_argument('--epoch', type=int, default=30, help='epoch')\n",
    "    parser.add_argument(\"--layer\", type=list, default=[8, 100, 80])\n",
    "    config = parser.parse_args()\n",
    "\n",
    "    dataloaders = classify_provider(features_path, label_path, n_splits, batch_size, num_workers)\n",
    "    for fold_index, [train_loader, valid_loader] in enumerate(dataloaders):\n",
    "        train_val = TrainVal(config, fold_index)\n",
    "        train_val.train(train_loader, valid_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
