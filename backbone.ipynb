{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-09T03:11:37.745436Z",
     "start_time": "2020-11-09T03:11:37.734405Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Yuan\\\\Desktop'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import time\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from tqdm import tqdm\n",
    "from scipy.fftpack import fft\n",
    "from matplotlib.pylab import mpl\n",
    "import csv\n",
    "\n",
    "\n",
    "%matplotlib qt5\n",
    "plt.rcParams['xtick.direction'] = 'in'\n",
    "plt.rcParams['ytick.direction'] = 'in'\n",
    "mpl.rcParams['axes.unicode_minus'] = False  #显示负号\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-09T06:40:39.537272Z",
     "start_time": "2020-11-09T06:40:39.513275Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def cal_linear(energy, inter, mid, interval_num, idx = 0):\n",
    "    # 初始化横坐标\n",
    "    x = np.array([])\n",
    "    for i in inter:\n",
    "        if i != 0:\n",
    "            x = np.append(x, np.linspace(i, i * 10, interval_num, endpoint=False))\n",
    "        else:\n",
    "            x = np.append(x, np.linspace(i, 1, interval_num, endpoint=False))\n",
    "    \n",
    "    # 初始化纵坐标\n",
    "    y = np.zeros(x.shape[0])\n",
    "    for i in energy:\n",
    "#         print(idx, x, y, i)\n",
    "        while True:\n",
    "            try:\n",
    "                if x[idx] <= i < x[idx + 1]:\n",
    "                    y[idx] += 1\n",
    "                    break\n",
    "            except IndexError:\n",
    "                if x[idx] <= i:\n",
    "                    y[idx] += 1\n",
    "                    break\n",
    "            idx += 1\n",
    "    \n",
    "    # 对横坐标作进一步筛选，计算概率分布值\n",
    "    x, y = x[y != 0], y[y != 0]\n",
    "    xx = np.zeros(x.shape[0])\n",
    "    yy = y / sum(y)\n",
    "    \n",
    "    # 取区间终点作为该段的横坐标\n",
    "    for idx in range(len(x) - 1):\n",
    "        xx[idx] = (x[idx] + x[idx + 1]) / 2\n",
    "    xx[-1] = x[-1]\n",
    "    \n",
    "    # 计算分段区间长度，从而求得概率密度值\n",
    "    interval = []\n",
    "    for i, j in enumerate(mid):\n",
    "        try:\n",
    "            num = len(np.intersect1d(np.where(inter[i] <= xx)[0], \n",
    "                                     np.where(xx < inter[i + 1])[0]))\n",
    "            interval.extend([j] * num)\n",
    "        except IndexError:\n",
    "            num = len(np.where(inter[i] <= xx)[0])\n",
    "            interval.extend([j] * num)\n",
    "    yy = yy / np.array(interval)\n",
    "    # 取对数变换为线性关系\n",
    "    xx = np.log10(xx)\n",
    "    yy = np.log10(yy)\n",
    "    fit = np.polyfit(xx, yy, 1)\n",
    "    alpha = abs(fit[0])\n",
    "    fit_x = np.linspace(min(xx), max(xx), 100)\n",
    "    fit_y = np.polyval(fit, fit_x)\n",
    "    return xx, yy, fit_x, fit_y, alpha\n",
    "\n",
    "\n",
    "def cal_PDF(tmp, inter, mid, interval_num):\n",
    "    ax = fig.add_subplot(331 + i)\n",
    "    xx, yy, fit_x, fit_y, alpha = cal_linear(sorted(tmp), inter, mid, interval_num)\n",
    "    tmp = np.append(xx, yy, axis=0)\n",
    "    with open(ylabel + '.txt', 'w') as f:\n",
    "        f.write('{}, {}\\n'.format(xlabel, ylabel))\n",
    "        for j in range(xx.shape[0]):\n",
    "            f.write('{}, {}\\n'.format(xx[j], yy[j]))\n",
    "    ax.scatter(xx, yy, edgecolors='blue')\n",
    "    ax.plot(fit_x, fit_y, label=r'$\\varepsilon$={:.2f}'.format(alpha))\n",
    "    \n",
    "#     tmp_1, tmp_2 = sorted(tmp[cls_1]), sorted(tmp[cls_2])\n",
    "#     xx, yy, fit_x, fit_y, alpha = cal_linear(tmp_1, inter, mid, interval_num)\n",
    "#     ax.scatter(xx, yy, edgecolors='purple')\n",
    "#     ax.plot(fit_x, fit_y, label=r'$\\varepsilon$={:.2f}'.format(alpha))\n",
    "#     xx, yy, fit_x, fit_y, alpha = cal_linear(tmp_2, inter, mid, interval_num)\n",
    "#     ax.scatter(xx, yy, edgecolors='g')\n",
    "#     ax.plot(fit_x, fit_y, label=r'$\\varepsilon$={:.2f}'.format(alpha))\n",
    "    ax.set_xlabel(xlabel)\n",
    "    ax.set_ylabel(ylabel)\n",
    "    ax.legend(loc='upper right')\n",
    "    \n",
    "\n",
    "def cal_ML(tmp, inter, mid, N, interval_num):\n",
    "    ax2 = fig.add_subplot(334 + i)\n",
    "#     tmp_1, tmp_2 = sorted(tmp[cls_1]), sorted(tmp[cls_2])\n",
    "#     N1, N2 = len(tmp_1), len(tmp_2)\n",
    "    ML_y1, ML_y2 = [], []\n",
    "    Error_bar1, Error_bar2 = [] ,[]\n",
    "    for j in tqdm(range(N)):\n",
    "        valid_x = sorted(tmp)[j:]\n",
    "        xx, yy, fit_x, fit_y, alpha = cal_linear(valid_x, inter, mid, interval_num)\n",
    "#         Sum = np.sum(valid_x)\n",
    "        N_prime = N - j\n",
    "#         alpha = N_prime / Sum + 1\n",
    "        error_bar = (alpha - 1) / pow(N_prime, 0.5)\n",
    "        ML_y1.append(alpha)\n",
    "        Error_bar1.append(error_bar)\n",
    "#     for j in tqdm(range(N2)):\n",
    "#         valid_x = sorted(tmp_1)[j:]\n",
    "#         xx, yy, fit_x, fit_y, alpha = cal_linear(valid_x, inter, mid, interval_num)\n",
    "#         N_prime = N - j\n",
    "#         error_bar = (alpha - 1) / pow(N_prime, 0.5)\n",
    "#         ML_y2.append(alpha)\n",
    "#         Error_bar2.append(error_bar)\n",
    "    \n",
    "    with open(xlabel[:-4] + '_ML.txt', 'w') as f:\n",
    "        f.write('{}, ε, Error bar\\n'.format(xlabel))\n",
    "        for j in range(len(ML_y1)):\n",
    "            f.write('{}, {}, {}\\n'.format(np.log10(sorted(tmp))[j], ML_y1[j], Error_bar1[j]))\n",
    "    ax2.errorbar(np.log10(sorted(tmp)), ML_y1, yerr=Error_bar1, \n",
    "                 fmt='o', ecolor='purple', color='purple', \n",
    "                 elinewidth=1, capsize=2, ms=5)\n",
    "#     ax2.errorbar(np.log10(tmp_2), ML_y2, yerr=Error_bar2, \n",
    "#                  fmt='o', ecolor='g', color='g', \n",
    "#                  elinewidth=1, capsize=2, ms=5)\n",
    "    ax2.set_xlabel(xlabel)\n",
    "    ax2.set_ylabel(r'$\\epsilon$')\n",
    "\n",
    "    \n",
    "def cal_correlation(feature):\n",
    "    cor_idx = [[0, 3], [2, 0], [2, 3]]\n",
    "    for idx, [i, j] in enumerate(cor_idx):\n",
    "        ax3 = fig.add_subplot(337 + idx)\n",
    "#         cor_x = np.log10(feature[:, i]) if i != 3 else np.log10(feature[:, i] * pow(10, 6))\n",
    "        cor_x = np.log10(feature[:, i])\n",
    "        cor_y = np.log10(feature[:, j])\n",
    "#         cor_x1, cor_x2 = cor_x[cls_1], cor_x[cls_2]\n",
    "#         cor_y1, cor_y2 = cor_y[cls_1], cor_y[cls_2]\n",
    "#         ax3.scatter(cor_x1, cor_y1, edgecolors='purple')\n",
    "#         ax3.scatter(cor_x2, cor_y2, edgecolors='g')\n",
    "        ax3.scatter(cor_x, cor_y, edgecolors='blue')\n",
    "        ax3.set_xlabel(xlabelz[max(0, i - 1)])\n",
    "        ax3.set_ylabel(xlabelz[max(0, j - 1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-09T06:40:46.200246Z",
     "start_time": "2020-11-09T06:40:41.265757Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|██████████████████████████████████████████████████████████████████████████▋    | 615/650 [00:01<00:00, 630.92it/s]<ipython-input-74-c1fe77a0316d>:88: RankWarning: Polyfit may be poorly conditioned\n",
      "  xx, yy, fit_x, fit_y, alpha = cal_linear(valid_x, inter, mid, interval_num)\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 650/650 [00:01<00:00, 522.93it/s]\n",
      " 84%|██████████████████████████████████████████████████████████████████▋            | 549/650 [00:01<00:00, 585.78it/s]<ipython-input-74-c1fe77a0316d>:88: RankWarning: Polyfit may be poorly conditioned\n",
      "  xx, yy, fit_x, fit_y, alpha = cal_linear(valid_x, inter, mid, interval_num)\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 650/650 [00:01<00:00, 538.24it/s]\n",
      " 87%|█████████████████████████████████████████████████████████████████████          | 568/650 [00:01<00:00, 531.65it/s]<ipython-input-74-c1fe77a0316d>:88: RankWarning: Polyfit may be poorly conditioned\n",
      "  xx, yy, fit_x, fit_y, alpha = cal_linear(valid_x, inter, mid, interval_num)\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 650/650 [00:01<00:00, 459.37it/s]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    path = r'C:\\Users\\Yuan\\Desktop'\n",
    "    os.chdir(path)\n",
    "    features_path = r'.\\pri_database.txt'\n",
    "#     r'D:\\data\\3D porous TC4-2mA-compression test-z1-0.001-20200929\\3D porous TC4-2mA-compression test-z1-0.001-20200929.txt'\n",
    "#     r'C:\\Users\\Yuan\\Desktop\\pri_database.txt'\n",
    "#     r'C:\\Users\\Yuan\\Desktop\\CM-4M-o18-2020.10.17-1-60.txt'\n",
    "    # r'E:\\data\\CM-PM-o18-2020.10.17\\CM-PM-o18-2020.10.17.txt'\n",
    "\n",
    "#     label_path = r'C:\\Users\\Yuan\\Desktop\\label.txt'\n",
    "\n",
    "    # Amp,RiseT,Dur,Eny,RMS,Counts\n",
    "    with open(features_path, 'r') as f:\n",
    "        feature = np.array([i.split(',')[6:-4] for i in f.readlines()[1:]])\n",
    "    feature = feature.astype(np.float32)\n",
    "    \n",
    "#     with open(label_path, 'r') as f:\n",
    "#         label = np.array([i.strip() for i in f.readlines()[1:]])\n",
    "#     label = label.astype(np.float32).reshape(-1, 1)\n",
    "#     label[np.where(label == 2)] = 0\n",
    "#     ext = np.zeros([feature.shape[0], 1])\n",
    "#     ext[np.where(label == 0)[0].tolist()] = 1\n",
    "#     label = np.concatenate((label, ext), axis=1)\n",
    "#     cls_1 = label[:, 0] == 1\n",
    "#     cls_2 = label[:, 1] == 1\n",
    "\n",
    "    feature_idx = [0, 2, 3]\n",
    "    N = feature.shape[0]\n",
    "    interval_num = 6\n",
    "    interval = 1 / interval_num\n",
    "    interz = []\n",
    "    midz = []\n",
    "\n",
    "    for idx in feature_idx:\n",
    "#         tmp = feature[:, idx] * pow(10, 6) if idx == 3 else feature[:, idx]\n",
    "        tmp = feature[:, idx]\n",
    "        tmp_max = int(max(tmp))\n",
    "        tmp_min = int(min(tmp))\n",
    "        if tmp_min <= 0:\n",
    "            interz.append([0] + [pow(10, i) for i in range(len(str(tmp_max)))])\n",
    "            midz.append([interval * pow(10, i)\n",
    "                         for i in range(len(str(tmp_max)) + 1)])\n",
    "        else:\n",
    "            interz.append([pow(10, i) for i in range(len(str(tmp_min)) - 1, \n",
    "                                                     len(str(tmp_max)))])\n",
    "            midz.append([interval * pow(10, i) \n",
    "                         for i in range(len(str(tmp_min)), \n",
    "                                        len(str(tmp_max)) + 1)])\n",
    "\n",
    "    xlabelz = ['Amplitude(μV)', 'Duration(μs)', 'Energy(aJ)']\n",
    "    ylabelz = ['PDF(A)', 'PDF(D)', 'PDF(E)']\n",
    "    \n",
    "    fig = plt.figure()\n",
    "    for i, [idx, inter, mid, xlabel, ylabel\n",
    "            ] in enumerate(zip(feature_idx, interz, midz, xlabelz, ylabelz)):\n",
    "#         tmp = feature[:, idx] * pow(10, 6) if idx == 3 else feature[:, idx]\n",
    "        tmp = feature[:, idx]\n",
    "#         tmp /= min(tmp)\n",
    "        cal_PDF(tmp, inter, mid, interval_num)\n",
    "        cal_ML(tmp, inter, mid, N, interval_num)\n",
    "    cal_correlation(feature)\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequency domain curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-06T07:44:50.837491Z",
     "start_time": "2020-11-06T07:44:14.291491Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Import data\n",
    "path = r'D:\\data\\3D porous TC4-2mA-compression test-z1-0.001-20200929'\n",
    "os.chdir(path)\n",
    "# E:\\data\\CM-PM-o18-2020.10.17\n",
    "# E:\\data\\CM-4M-o18-2020.10.17-1-60\n",
    "convert_path = path.split('\\\\')[-1] + '.txt'\n",
    "ls = os.listdir(path)[1:]\n",
    "lss = np.array(['_'.join(i.split('_')[1:3]) for i in ls])\n",
    "file = []\n",
    "with open(convert_path, \"r\") as f:\n",
    "    f.readline()\n",
    "    valid_idx = np.array([\"_\".join([i.split(',')[2].strip(), \n",
    "                                    i.split(',')[0].strip()]) \n",
    "                          for i in f.readlines()])\n",
    "with open(convert_path, \"r\") as f:\n",
    "    f.readline()\n",
    "    energy = np.array([float(i.split(',')[-3].strip()) for i in f.readlines()])\n",
    "for i in tqdm(valid_idx):\n",
    "    file.append(path + '\\\\' + ls[np.where(lss == i)[0][0]])\n",
    "# file = np.array([path + '\\\\' + ls[np.where(lss == i)[0][0]] for i in valid_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate frequency\n",
    "freq_max = [[]] * len(file)\n",
    "thr_V = pow(10, 25 / 20) / pow(10, 3)\n",
    "\n",
    "for idx, i in enumerate(tqdm(file)):\n",
    "    with open(i, \"r\") as f:\n",
    "        for _ in range(10):\n",
    "            f.readline()\n",
    "        t0 = float(f.readline()[14:])\n",
    "        amp = np.array([float(i.strip(\"\\n\")) for i in f.readlines()[1:]])\n",
    "#     time[idx] = np.linspace(t0, t0 + 0.0000002 * (amp.shape[0] - 1), amp.shape[0])\n",
    "#     Amp[idx] = amp[:]\n",
    "    \n",
    "    valid_wave_idx = np.where(abs(amp) >= thr_V)[0]\n",
    "    valid_data = amp[valid_wave_idx[0]:(valid_wave_idx[-1] + 1)]\n",
    "\n",
    "    Ts = 0.0000002\n",
    "    Fs = 1 / Ts\n",
    "    N = valid_wave_idx[-1] - valid_wave_idx[0] + 1\n",
    "    end = Ts * N\n",
    "    time_label = np.arange(0, end, Ts)\n",
    "    frq = (np.arange(N) / N) * Fs\n",
    "    fft_y = fft(valid_data)\n",
    "\n",
    "    abs_y = np.abs(fft_y)\n",
    "    normalization = abs_y / N\n",
    "    half_frq = frq[range(int(N / 2))]\n",
    "    normalization_half = normalization[range(int(N / 2))]\n",
    "    freq_max[idx] = half_frq[np.argmax(normalization_half)]\n",
    "\n",
    "#     # Plot\n",
    "#     titles = ['Original Waveform', 'Bilateral amplitude spectrum (normalized)', 'Unilateral amplitude spectrum (normalized)']\n",
    "#     colors = ['purple', 'green', 'blue']\n",
    "#     x_label = ['Time (s)', 'Freq (Hz)', 'Freq (Hz)']\n",
    "#     y_label = ['Amplitude (μV)', '|Y(freq)|', '|Y(freq)|']\n",
    "#     xs = [time_label, frq, half_frq]\n",
    "#     ys = [valid_data, normalization, normalization_half]\n",
    "\n",
    "#     for i, [x, y, title, color, xlabel, ylabel] in enumerate(zip(xs, ys, titles, colors, x_label, y_label)):\n",
    "#         plt.subplot(311 + i)\n",
    "#         plt.plot(x, y, color)\n",
    "#         plt.xlabel(xlabel)\n",
    "#         plt.ylabel(ylabel)\n",
    "#         plt.title(title, color=color)\n",
    "#     plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-06T04:14:29.665472Z",
     "start_time": "2020-11-06T03:41:33.352453Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 84641/84641 [32:56<00:00, 42.83it/s]\n"
     ]
    }
   ],
   "source": [
    "All_file = os.listdir(path)[2:]\n",
    "# time, Amp = [[]] * len(All_file), [[]] * len(All_file)\n",
    "# for idx, i in enumerate(tqdm(All_file)):\n",
    "#     with open(i, \"r\") as f:\n",
    "#         for _ in range(10):\n",
    "#             f.readline()\n",
    "#         t0 = float(f.readline()[14:])\n",
    "#         amp = max(np.array([float(i.strip(\"\\n\")) for i in f.readlines()[1:]]))\n",
    "#     time = np.append(time, t0)\n",
    "#     Amp = np.append(Amp, amp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-06T02:31:58.138600Z",
     "start_time": "2020-11-06T02:31:58.124567Z"
    }
   },
   "outputs": [],
   "source": [
    "# # Save the calculated frequency value in CSV format\n",
    "# file_name = path.split('\\\\')[-1]\n",
    "# with open(file_name + '-frequency.csv', 'w') as csvfile:\n",
    "#     writer = csv.writer(csvfile)\n",
    "#     writer.writerows([freq_max])\n",
    "\n",
    "# # Change the frequency file format to TXT\n",
    "# os.rename(path + '\\\\' + file_name + '-frequency.csv', \n",
    "#           path + '\\\\' + file_name + '-frequency.txt')\n",
    "# with open(file_name + '-frequency.txt', 'r') as f:\n",
    "#     data = f.readlines()[:-1]\n",
    "# freq_max = [i.split(',') for i in data]\n",
    "# freq_max = np.array(freq_max[0]).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-06T06:13:41.324782Z",
     "start_time": "2020-11-06T06:13:38.861845Z"
    }
   },
   "outputs": [],
   "source": [
    "# Plot the frequency energy map\n",
    "plt.scatter(energy, freq_max)\n",
    "plt.xlim(-10, math.ceil(max(energy)))\n",
    "plt.xlabel('Energy(aJ)')\n",
    "plt.ylabel('Frequency(Hz)')\n",
    "plt.title('Frequency-Energy')\n",
    "\n",
    "# plt.bar(time, Amp)\n",
    "# plt.xlim(math.floor(min(time)), math.ceil(max(time)))\n",
    "# plt.xlabel('Time(s)')\n",
    "# plt.ylabel('Amplitude(μV)')\n",
    "# plt.title('Time-Amplitude')\n",
    "plt.savefig(r'C:\\Users\\Yuan\\Desktop\\test.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-02T09:29:26.691454Z",
     "start_time": "2020-11-02T09:29:26.671424Z"
    }
   },
   "outputs": [],
   "source": [
    "# xx[-1]\n",
    "# 9, 5, 4, 2, 1, 0.8, 0.75, 0.5, 0.5\n",
    "base = np.array([9, 14, 18, 20, 21, 21.8, 22.55, 23.05, 23.55])\n",
    "tick_1 = base + 0\n",
    "tick_2 = base + tick_1[-1]\n",
    "tick_3 = base + tick_2[-1]\n",
    "tick_4 = base + tick_3[-1]\n",
    "tick_5 = base + tick_4[-1]\n",
    "tick_6 = base + tick_5[-1]\n",
    "x_tick = np.concatenate((tick_1, tick_2, tick_3, tick_4, tick_5, tick_6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time domain curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-30T00:42:58.921178Z",
     "start_time": "2020-10-30T00:42:58.866841Z"
    }
   },
   "outputs": [],
   "source": [
    "pca = PCA(n_components=5)\n",
    "newData = pca.fit_transform(feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-30T01:00:11.470400Z",
     "start_time": "2020-10-30T01:00:11.462370Z"
    }
   },
   "outputs": [],
   "source": [
    "pca.components_.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import time\n",
    "import argparse\n",
    "import torch\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-30T02:58:54.931743Z",
     "start_time": "2020-10-30T02:58:54.906715Z"
    }
   },
   "outputs": [],
   "source": [
    "def metric(logit, truth, threshold=0.5):\n",
    "    batch_size, num_class = logit.shape\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logit = logit.view(batch_size, num_class, -1)\n",
    "        truth = truth.view(batch_size, num_class, -1)\n",
    "\n",
    "        probability = torch.sigmoid(logit)\n",
    "        p = (probability > threshold).float()\n",
    "        t = (truth > 0.5).float()\n",
    "\n",
    "        tp = ((p + t) == 2).float()  # True positives\n",
    "        tn = ((p + t) == 0).float()  # True negatives\n",
    "        # 各个类别预测正确的正样本、负样本数目\n",
    "        tp = tp.sum(dim=0)\n",
    "        tn = tn.sum(dim=0)\n",
    "        num_pos = t.sum(dim=0)\n",
    "        num_neg = batch_size - num_pos\n",
    "        # 预测正确的正样本和负样本的数目\n",
    "        tp = tp.data.cpu().numpy()\n",
    "        tn = tn.data.cpu().numpy()\n",
    "        # 正样本、负样本的数目\n",
    "        num_pos = num_pos.data.cpu().numpy()\n",
    "        num_neg = num_neg.data.cpu().numpy()\n",
    "\n",
    "        # tp = np.nan_to_num(tp / (num_pos + 1e-12), 0)\n",
    "        # tn = np.nan_to_num(tn / (num_neg + 1e-12), 0)\n",
    "\n",
    "        # tp = list(tp)\n",
    "        # num_pos = list(num_pos)\n",
    "\n",
    "    return tn, tp, num_neg, num_pos\n",
    "\n",
    "\n",
    "class Meter:\n",
    "    '''A meter to keep track of iou and dice scores throughout an epoch'''\n",
    "    def __init__(self):\n",
    "        self.base_threshold = 0.5\n",
    "        self.true_negative = []\n",
    "        self.true_poisitive = []\n",
    "        self.number_negative = []\n",
    "        self.number_positive = []\n",
    "\n",
    "    def update(self, targets, outputs):\n",
    "        tn, tp, num_neg, num_pos = metric(outputs, targets, self.base_threshold)\n",
    "        self.true_negative.append(tn)\n",
    "        self.true_poisitive.append(tp)\n",
    "        self.number_negative.append(num_neg)\n",
    "        self.number_positive.append(num_pos)\n",
    "\n",
    "    def get_metrics(self):\n",
    "        # 各类预测正确的样本数目，样本总数目\n",
    "        class_tn = np.sum(np.array(self.true_negative), axis=0)\n",
    "        class_tp = np.sum(np.array(self.true_poisitive), axis=0)\n",
    "        class_num_neg = np.sum(np.array(self.number_negative), axis=0)\n",
    "        class_num_pos = np.sum(np.array(self.number_positive), axis=0)\n",
    "        # 预测正确的样本的总数目，样本总数目\n",
    "        tn = np.sum(self.true_negative)\n",
    "        tp = np.sum(self.true_poisitive)\n",
    "        num_neg = np.sum(self.number_negative)\n",
    "        num_pos = np.sum(self.number_positive)\n",
    "        # 各类的正负样本的准确率和总的准确率\n",
    "        class_neg_accuracy = class_tn / class_num_neg\n",
    "        class_pos_accuracy = class_tp / class_num_pos\n",
    "        class_accuracy = (class_tn + class_tp) / (class_num_neg + class_num_pos)\n",
    "        # 正负样本各自的准确率和总的准确率\n",
    "        neg_accuracy = tn / (num_neg + 1e-12)\n",
    "        pos_accuracy = tp / (num_pos + 1e-12)\n",
    "        accuracy = (tn + tp) / (num_neg + num_pos)\n",
    "\n",
    "        return class_neg_accuracy, class_pos_accuracy, class_accuracy, neg_accuracy, pos_accuracy, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-30T02:31:26.350549Z",
     "start_time": "2020-10-30T02:31:26.331555Z"
    }
   },
   "outputs": [],
   "source": [
    "class Classify_model(torch.nn.Module):\n",
    "    def __init__(self, layer, training=True):\n",
    "        super(Fit_model,self).__init__()\n",
    "        self.linear1 = torch.nn.Linear(layer[0],layer[1])\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.linear2 = torch.nn.Linear(layer[1],layer[2])\n",
    "        self.linear3 = torch.nn.Linear(layer[2],1)\n",
    "        self.criterion = torch.nn.MSELoss()\n",
    "        self.sigmoid = torch.nn.Sigmoid() \n",
    "        self.opt = torch.optim.SGD(self.parameters(),lr=0.0001)\n",
    "        self.training = training\n",
    "    def forward(self, input):\n",
    "        y = self.linear1(input)\n",
    "        y = self.relu(y)\n",
    "        y = F.dropout(y, 0.5, training=self.training)\n",
    "        y = self.linear2(y)\n",
    "        y = self.sigmoid(y)\n",
    "        y = self.linear3(y)\n",
    "        y = self.sigmoid(y)\n",
    "        return y "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-30T01:54:17.434672Z",
     "start_time": "2020-10-30T01:54:17.424642Z"
    }
   },
   "outputs": [],
   "source": [
    "class SteelClassDataset(Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        super(SteelClassDataset, self).__init__()\n",
    "        self.feature = dataset[0]\n",
    "        self.label = dataset[1]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.feature[idx]\n",
    "        y = self.label[idx]\n",
    "        return x, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-30T02:46:37.616990Z",
     "start_time": "2020-10-30T02:46:37.593021Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def classify_provider(features_path, label_path, n_splits, batch_size,num_workers):\n",
    "    # Time,Thr,Amp,RiseT,Dur,Eny,RMS,Counts\n",
    "    with open(features_path, 'r') as f:\n",
    "        feature = np.array([i.split(',')[6:-4] for i in f.readlines()[1:]])\n",
    "    # feature = np.delete(feature, [1, 2], 1).astype(np.float32)\n",
    "    feature = torch.from_numpy(feature.astype(np.float32))\n",
    "\n",
    "    with open(label_path, 'r') as f:\n",
    "        label = np.array([i.strip() for i in f.readlines()[1:]])\n",
    "    label = label.astype(np.float32)\n",
    "    label[np.where(label == 2)] = 0\n",
    "    label = torch.unsqueeze(torch.from_numpy(label), dim=1)\n",
    "    \n",
    "    train_dfs = list()\n",
    "    val_dfs = list()\n",
    "    if n_splits != 1:\n",
    "        skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=69)\n",
    "        for train_df_index, val_df_index in skf.split(feature, label):\n",
    "            train_dfs.append([feature[train_df_index], label[train_df_index]])\n",
    "            val_dfs.append([feature[val_df_index], label[val_df_index]])\n",
    "    else:\n",
    "        df_temp = train_test_split(feature,label, test_size=0.2, stratify=label, random_state=69)\n",
    "        train_dfs.append([df_temp[0], df_temp[2]])\n",
    "        val_dfs.append([df_temp[1], df_temp[3]])\n",
    "\n",
    "    dataloaders = list()\n",
    "    for df_index, (train_df, val_df) in enumerate(zip(train_dfs, val_dfs)):\n",
    "        train_dataset = SteelClassDataset(train_df)\n",
    "        val_dataset = SteelClassDataset(val_df)\n",
    "        train_dataloader = DataLoader(train_dataset,\n",
    "                                      batch_size=batch_size,\n",
    "                                      num_workers=num_workers,\n",
    "                                      pin_memory=True,\n",
    "                                      shuffle=True)\n",
    "        val_dataloader = DataLoader(val_dataset,\n",
    "                                    batch_size=batch_size,\n",
    "                                    num_workers=num_workers,\n",
    "                                    pin_memory=True,\n",
    "                                    shuffle=False)\n",
    "        dataloaders.append([train_dataloader, val_dataloader])\n",
    "    return dataloaders\n",
    "\n",
    "\n",
    "class Solver():\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.device = torch.device(\n",
    "            'cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    def forward(self, images):\n",
    "        images = images.to(self.device)\n",
    "        outputs = self.model(images)\n",
    "        return outputs\n",
    "\n",
    "    def cal_loss(self, targets, predicts, criterion):\n",
    "        targets = targets.to(self.device)\n",
    "        return criterion(predicts, targets)\n",
    "\n",
    "    def backword(self, optimizer, loss):\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    def save_checkpoint(self, save_path, state, is_best):\n",
    "        torch.save(state, save_path)\n",
    "        if is_best:\n",
    "            print('Saving Best Model.')\n",
    "            save_best_path = save_path.replace('.pth', '_best.pth')\n",
    "            shutil.copyfile(save_path, save_best_path)\n",
    "\n",
    "    def load_checkpoint(self, load_path):\n",
    "        if os.path.isfile(load_path):\n",
    "            checkpoint = torch.load(load_path, map_location='cpu')\n",
    "            # self.model.module.load_state_dict(checkpoint['state_dict'])\n",
    "            print('Successfully Loaded from %s' % (load_path))\n",
    "            return self.model\n",
    "        else:\n",
    "            raise FileNotFoundError(\n",
    "                \"Can not find weight file in {}\".format(load_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-30T02:47:08.770237Z",
     "start_time": "2020-10-30T02:47:08.752270Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class TrainVal():\n",
    "    def __init__(self, config, fold):\n",
    "        self.model = Classify_model(config.layer, training=True)\n",
    "        if torch.cuda.is_available():\n",
    "            self.model = torch.nn.DataParallel(self.model)\n",
    "            self.model = self.model.cuda()\n",
    "        \n",
    "        self.lr = config.lr\n",
    "        self.weight_decay = config.weight_decay\n",
    "        self.epoch = config.epoch\n",
    "        self.fold = fold\n",
    "\n",
    "        self.solver = Solver(self.model)\n",
    "        self.criterion = torch.nn.MSELoss()\n",
    "        \n",
    "        self.model_path = config.save_path\n",
    "        if not os.path.exists(self.model_path):\n",
    "            os.makedirs(self.model_path)\n",
    "            \n",
    "    def train(self, train_loader, valid_loader):\n",
    "        optimizer = optim.Adam(self.model.module.parameters(), self.lr, weight_decay=self.weight_decay)\n",
    "        lr_scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, self.epoch+10)\n",
    "        global_step = 0\n",
    "\n",
    "        for epoch in range(self.epoch):\n",
    "            epoch += 1\n",
    "            epoch_loss = 0\n",
    "            self.model.train(True)\n",
    "\n",
    "            tbar = tqdm.tqdm(train_loader)\n",
    "            for i, (x, labels) in enumerate(tbar):\n",
    "                labels_predict = self.solver.forward(x)\n",
    "                loss = self.solver.cal_loss(labels, labels_predict, self.criterion)\n",
    "                epoch_loss += loss.item()\n",
    "                self.solver.backword(optimizer, loss)\n",
    "                \n",
    "                params_groups_lr = str()\n",
    "                for group_ind, param_group in enumerate(optimizer.param_groups):\n",
    "                    params_groups_lr = params_groups_lr + 'params_group_%d' % (group_ind) + ': %.12f, ' % (param_group['lr'])\n",
    "                descript = \"Fold: %d, Train Loss: %.7f, lr: %s\" % (self.fold, loss.item(), params_groups_lr)\n",
    "                tbar.set_description(desc=descript)\n",
    "            \n",
    "            lr_scheduler.step()\n",
    "            global_step += len(train_loader)\n",
    "\n",
    "            print('Finish Epoch [%d/%d], Average Loss: %.7f' % (epoch, self.epoch, epoch_loss/len(tbar)))\n",
    "            \n",
    "            class_neg_accuracy, class_pos_accuracy, class_accuracy, neg_accuracy, pos_accuracy, accuracy, loss_valid = \\\n",
    "                self.validation(valid_loader)\n",
    "\n",
    "            if accuracy > self.max_accuracy_valid: \n",
    "                is_best = True\n",
    "                self.max_accuracy_valid = accuracy\n",
    "            else:\n",
    "                is_best = False\n",
    "            \n",
    "            state = {\n",
    "                'epoch': epoch,\n",
    "                'state_dict': self.model.module.state_dict(),\n",
    "                'max_accuracy_valid': self.max_accuracy_valid,\n",
    "            }\n",
    "            \n",
    "            self.solver.save_checkpoint(os.path.join(self.model_path, '%s_classify_fold%d.pth' % (self.model_name, self.fold)), state, is_best)\n",
    "            self.writer.add_scalar('valid_loss', loss_valid, epoch)\n",
    "            self.writer.add_scalar('valid_accuracy', accuracy, epoch)\n",
    "            self.writer.add_scalar('valid_class_0_accuracy', class_accuracy[0], epoch)\n",
    "            self.writer.add_scalar('valid_class_1_accuracy', class_accuracy[1], epoch)\n",
    "            \n",
    "    def validation(self, valid_loader):\n",
    "        self.model.eval()\n",
    "        meter = Meter()\n",
    "        tbar = tqdm.tqdm(valid_loader)\n",
    "        loss_sum = 0\n",
    "        \n",
    "        with torch.no_grad(): \n",
    "            for i, (x, labels) in enumerate(tbar):\n",
    "                labels_predict = self.solver.forward(x)\n",
    "                loss = self.solver.cal_loss(labels, labels_predict, self.criterion)\n",
    "                loss_sum += loss.item()\n",
    "\n",
    "                meter.update(labels, labels_predict.cpu())\n",
    "\n",
    "                descript = \"Val Loss: {:.7f}\".format(loss.item())\n",
    "                tbar.set_description(desc=descript)\n",
    "        loss_mean = loss_sum / len(tbar)\n",
    "        \n",
    "        class_neg_accuracy, class_pos_accuracy, class_accuracy, neg_accuracy, pos_accuracy, accuracy = meter.get_metrics()\n",
    "        print(\"Class_0_accuracy: %0.4f | Class_1_accuracy: %0.4f | Negative accuracy: %0.4f | positive accuracy: %0.4f | accuracy: %0.4f\" %\n",
    "              (class_accuracy[0], class_accuracy[1], neg_accuracy, pos_accuracy, accuracy))\n",
    "        return class_neg_accuracy, class_pos_accuracy, class_accuracy, neg_accuracy, pos_accuracy, accuracy, loss_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-30T02:59:36.525403Z",
     "start_time": "2020-10-30T02:59:36.496375Z"
    }
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--features_path', type=str, default=r'C:\\Users\\Yuan\\Desktop\\pri_database.txt')\n",
    "    parser.add_argument('--label_path', type=str, default=r'C:\\Users\\Yuan\\Desktop\\label.txt')\n",
    "    parser.add_argument('--save_path', type=str, default='./checkpoints')\n",
    "    parser.add_argument('--class_num', type=int, default=4)\n",
    "    parser.add_argument('--num_workers', type=int, default=8)\n",
    "    parser.add_argument('--lr', type=float, default=5e-5, help='init lr')\n",
    "    parser.add_argument('--weight_decay', type=float, default=0, help='weight_decay in optimizer')\n",
    "    parser.add_argument('--n_splits', type=int, default=5, help='n_splits_fold')\n",
    "    parser.add_argument('--batch_size', type=int, default=24, help='batch size')\n",
    "    parser.add_argument('--epoch', type=int, default=30, help='epoch')\n",
    "    parser.add_argument(\"--layer\", type=list, default=[8, 100, 80])\n",
    "    config = parser.parse_args()\n",
    "\n",
    "    dataloaders = classify_provider(features_path, label_path, n_splits, batch_size, num_workers)\n",
    "    for fold_index, [train_loader, valid_loader] in enumerate(dataloaders):\n",
    "        train_val = TrainVal(config, fold_index)\n",
    "        train_val.train(train_loader, valid_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
